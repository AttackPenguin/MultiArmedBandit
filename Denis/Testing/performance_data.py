from __future__ import annotations

import os
import pickle
import sys
from typing import Type

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import torch

from Denis.nn_models import MABInceptionModel2
from Denis.reward_generators import RewardGenerator
from Denis.Testing.testing_generators import get_squished_test_gens

PICKLED_DIR = "pickled_data"

sns.set()


def main():
    dttm_started = pd.Timestamp.now()
    print(f"Started at {dttm_started}...")

    performance_data = get_training_performance(
        "/home/denis/PycharmProjects/MultiArmedBandit/Denis/Experiment 02/2022-03-04 15:27:08",
        MABInceptionModel2,
        get_squished_test_gens()
    )

    dttm_finished = pd.Timestamp.now()
    print(f"Finished at {dttm_finished}...")
    print(f"Elapsed time: {dttm_finished-dttm_started}")
    sys.exit(0)


def get_performance_on_generator(
        generator: RewardGenerator,
        nn: torch.nn.Module,
        n: int = 10,
        pulls: int = 100
):
    opt_single_reward = generator.get_max_mean()

    nn.eval()
    _, levers, rewards = nn([generator])
    levers = levers[0],
    rewards = rewards[0]
    overall_performance = (
            sum(rewards) / (opt_single_reward * pulls)
    )
    pull_performance = [
        reward / opt_single_reward for reward in rewards
    ]

    return (
        levers,  # A list of levers pulled by round
        rewards,  # A list of the rewards generated by round
        opt_single_reward,  # The mean reward of lever with the highest mean
        overall_performance,  # The actual total score / theoretical optimal
        pull_performance  # list of lever rewards / theoretical optimal
    )


def get_training_performance(
        dir_path: str,
        nn: torch.nn.Module,
        generators: list[RewardGenerator],
        use_pickled: bool = True
):
    pickled_path = (
        f"get_training_performance_"
        f"{dir_path[-19:]}.pickle"
    )
    pickled_path = os.path.join(
        PICKLED_DIR, pickled_path
    )

    if os.path.exists(pickled_path) and use_pickled:
        with open(pickled_path, 'rb') as file:
            performance_data = pickle.load(file)

    else:
        locs_file_path = os.path.join(
            dir_path, 'best_weights_locations.pickle'
        )
        with open(locs_file_path, 'rb') as file:
            locations = pickle.load(file)

        weights_files = list()
        for file_name in os.listdir(dir_path):
            if file_name.endswith('pth'):
                weights_files.append(file_name)

        performance_data = dict()
        for location, file_name in zip(locations, weights_files):
            list_levers = list()
            list_rewards = list()
            list_opt_single_reward = list()
            list_overall_performance = list()
            list_pull_performance = list()

            model = nn()
            model.load_state_dict(
                torch.load(os.path.join(dir_path, file_name))
            )
            model.eval()
            for generator in generators:
                (levers, rewards, opt_single_reward,
                 overall_performance, pull_performance) = \
                    get_performance_on_generator(
                        generator, model
                    )
                list_levers.append(levers)
                list_rewards.append(rewards)
                list_opt_single_reward.append(opt_single_reward)
                list_overall_performance.append(overall_performance)
                list_pull_performance.append(pull_performance)
            performance_data[location] = {
                'levers': list_levers,
                'rewards': list_rewards,
                'opt_single_reward': list_opt_single_reward,
                'overall_performance': list_overall_performance,
                'pull_performance': list_pull_performance
            }

        with open(pickled_path, 'wb') as file:
            pickle.dump(performance_data, file)

    return performance_data


if __name__ == '__main__':
    main()
